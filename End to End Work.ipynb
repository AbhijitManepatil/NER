{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53de8bbe",
   "metadata": {},
   "source": [
    "### import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c262e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a759c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d4fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\r\n",
      "============================== Info about spaCy ==============================\u001b[0m\r\n",
      "\r\n",
      "spaCy version    3.2.3                         \r\n",
      "Location         /home/user/anaconda3/envs/NER/lib/python3.8/site-packages/spacy\r\n",
      "Platform         Linux-5.13.0-40-generic-x86_64-with-glibc2.17\r\n",
      "Python version   3.8.12                        \r\n",
      "Pipelines        en_core_web_sm (3.2.0), en_core_web_lg (3.2.0), en_core_web_md (3.2.0), en_core_web_trf (3.2.0)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c4b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load small model\n",
    "nlp_sm_model=spacy.load('en_core_web_sm')\n",
    "#load medium spacy model\n",
    "nlp_md_model=spacy.load('en_core_web_md')\n",
    "#load large spacy model\n",
    "nlp_lg_model=spacy.load('en_core_web_lg')\n",
    "#load pre-trained transformer\n",
    "nlp_trf_model=spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c385c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b0a5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Custom Rulebased Skill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d81bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading full_matcher ...\n",
      "loading abv_matcher ...\n",
      "loading full_uni_matcher ...\n",
      "loading low_form_matcher ...\n",
      "loading token_matcher ...\n"
     ]
    }
   ],
   "source": [
    "#### Custom Rulebased Skill \n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "# load default skills data base\n",
    "from skillNer.general_params import SKILL_DB\n",
    "# import skill extractor\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "\n",
    "from spacy.language import Language\n",
    "# init params of skill extractor\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "# init skill extractor\n",
    "skill_extractor = SkillExtractor(nlp_trf_model, SKILL_DB, PhraseMatcher)\n",
    "\n",
    "# @Language.component(\"rulebasedSkills\")\n",
    "def rulebasedSkills(doc):\n",
    "    skills=[]\n",
    "    SkillResult={}\n",
    "    \n",
    "    annotations = skill_extractor.annotate(doc)\n",
    "    full_matches=annotations['results']['full_matches']\n",
    "    ngram_scored=annotations['results']['ngram_scored']\n",
    "\n",
    "    \n",
    "    for fm in full_matches:\n",
    "        skills.append(fm['doc_node_value'])\n",
    "    for ns in ngram_scored:\n",
    "        skills.append(ns['doc_node_value'])\n",
    "    \n",
    "    SkillResult['RuleSkills']=skills\n",
    "    \n",
    "    return SkillResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73db8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"\n",
    "You are a Python developer with a solid experience in web development\n",
    "and can manage projects. You quickly adapt to new environments\n",
    "and speak fluently English and French\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8d69d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result=rulebasedSkills(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0e3db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1f26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load custom trained model\n",
    "custnlp = spacy.load(\"./training/model-best\")\n",
    "# custnlp = spacy.load(\"./MytrainingMay10/model-best\")\n",
    "\n",
    "# (\"./training/model-best\")\n",
    "# custnlp=spacy.load(\"./Mytraining/model-best\")\n",
    "# - ner = source_nlp.get_pipe(\"ner\")\n",
    "# - nlp.add_pipe(ner)\n",
    "# nlp.add_pipe(\"ner\", source=source_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8678866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp_lg_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db5d5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f0afd7430b0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_lg_model.remove_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51b9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "print(nlp_lg_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060f17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(custnlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96caddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/NER/lib/python3.8/site-packages/spacy/language.py:710: UserWarning: [W113] Sourced component 'ner' may not work as expected: source vectors are not identical to current pipeline vectors.\n",
      "  warnings.warn(Warnings.W113.format(name=source_name))\n"
     ]
    }
   ],
   "source": [
    "ner=nlp_lg_model.add_pipe(\"ner\", source=custnlp, after=\"tok2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a3f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/user/anaconda3/envs/NER/lib/python3.8/site-packages/spacy/language.py:710: \n",
    "# UserWarning: [W113] Sourced component 'ner' may not work as expected: source vectors are not identical \n",
    "# to current pipeline vectors.\n",
    "#   warnings.warn(Warnings.W113.format(name=source_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d268e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "print(nlp_lg_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a58e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9f61fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'rulebasedSkills']\n"
     ]
    }
   ],
   "source": [
    "### Add rule based skilled \n",
    "\n",
    "nlp_trf_model.add_pipe(\"rulebasedSkills\")\n",
    "# nlp.add_pipe(\"ner\", source=other_nlp)\n",
    "print(nlp_trf_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b353d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_trf_model.remove_pipe(\"test2_component\")\n",
    "# print(nlp_trf_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc2bfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TEST on DATA\n",
    "# import docx2txt\n",
    "# Cv_PathToTest=\"./ResumeData/CV Wyc.docx\"\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# Cv_PathToTest=\"./ResumeData/RMS_DATA/Resume_Download/1Mawanda Swaib CV.docx\" #select resume to test\n",
    "\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "text_file = open(\"/home/user/HR Analytics/Custom_NER/testResume.txt\",\"r\")\n",
    "cv_text = text_file.read()\n",
    "# print(data)\n",
    "\n",
    "doc = nlp_lg_model(cv_text) # input sample text\n",
    "# print(\"====\",doc.ents)\n",
    "# check with only entity result \n",
    "for word in doc.ents:\n",
    "    print(word.text,\"==========>\",word.label_)\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48324b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lls\"], [1371, 1380, \"skills\"], [487, 489, \"degree\"], [52, 68, \"skills\"], [106, 115, \"skills\"], [117, 129, \"skills\"], [131, 135, \"skills\"], [136, 145, \"skills\"], [170, 171, \"degree\"], [173, 176, \"skills\"], [187, 190, \"skills\"], [670, 672, \"degree\"], [1104, 1109, \"skills\"], [1450, 1453, \"skills\"], [1491, 1498, \"skills\"], [1562, 1590, \"company_names\"], [1733, 1752, \"company_names\"], [1248, 1256, \"Location\"], [1178, 1183, \"Location\"], [1362, 1370, \"Total_Experience\"], [1393, 1402, \"Total_Experience\"], [1436, 1442, \"Total_Experience\"], [1477, 1483, \"Total_Experience\"], [1522, 1528, \"Total_Experience\"], [659, 669, \"skills\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b17a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' - Les'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_text[1522:1528]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "81f88caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== (Coordinated, Coordinated)\n",
      "Coordinated ==========> Location\n",
      "Coordinated ==========> Location\n"
     ]
    }
   ],
   "source": [
    "##### TEST on DATA\n",
    "import docx2txt\n",
    "Cv_PathToTest=\"./ResumeData/CV Wyc.docx\"\n",
    "cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# Cv_PathToTest=\"./ResumeData/RMS_DATA/Resume_Download/1Mawanda Swaib CV.docx\" #select resume to test\n",
    "\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# text_file = open(\"/home/user/HR Analytics/Custom_NER/Java_Developer/Java_Developer_1.txt\",\"r\")\n",
    "# cv_text = text_file.read()\n",
    "# print(data)\n",
    "\n",
    "doc = nlp_lg_model(cv_text) # input sample text\n",
    "# print(\"====\",doc.ents)\n",
    "# check with only entity result \n",
    "for word in doc.ents:\n",
    "    print(word.text,\"==========>\",word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "07ac5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== ()\n"
     ]
    }
   ],
   "source": [
    "##### TEST on DATA\n",
    "import docx2txt\n",
    "Cv_PathToTest=\"./ResumeData/CV Wyc.docx\"\n",
    "cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# Cv_PathToTest=\"./ResumeData/RMS_DATA/Resume_Download/1Mawanda Swaib CV.docx\" #select resume to test\n",
    "\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# text_file = open(\"/home/user/HR Analytics/Custom_NER/Java_Developer/Java_Developer_1.txt\",\"r\")\n",
    "# cv_text = text_file.read()\n",
    "# print(data)\n",
    "\n",
    "doc = nlp_lg_model(cv_text) # input sample text\n",
    "print(\"====\",doc.ents)\n",
    "# check with only entity result \n",
    "for word in doc.ents:\n",
    "    print(word.text,\"==========>\",word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_nlp.add_pipe(\n",
    "    \"ner\",\n",
    "    name=\"ner_drug\",\n",
    "    source=custnlp,\n",
    "    after=\"ner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7608914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RuleSkills': ['supply chain',\n",
       "  'supply chain management',\n",
       "  'supply chain',\n",
       "  'acquisition process',\n",
       "  'resource planning',\n",
       "  'acquisition process',\n",
       "  'asset management',\n",
       "  'fleet management',\n",
       "  'supply chain',\n",
       "  'supply chain management',\n",
       "  'international purchasing',\n",
       "  'supply chain',\n",
       "  'supply chain management',\n",
       "  'business administration',\n",
       "  'supply chain',\n",
       "  'contract negotiation',\n",
       "  'supply chain',\n",
       "  'payment processing',\n",
       "  'system development',\n",
       "  'project management',\n",
       "  'acquisition process',\n",
       "  'tax assessment',\n",
       "  'custom law',\n",
       "  'contract negotiation',\n",
       "  'market research',\n",
       "  'earn value management',\n",
       "  'product quality',\n",
       "  'quality assurance',\n",
       "  'supply chain',\n",
       "  'cost control',\n",
       "  'acquisition process',\n",
       "  'supply chain',\n",
       "  'information technology',\n",
       "  'computer network',\n",
       "  'key performance indicator',\n",
       "  'supply chain',\n",
       "  'project management',\n",
       "  'shipping management',\n",
       "  'supply chain',\n",
       "  'international trade',\n",
       "  'system integration',\n",
       "  'contract negotiation',\n",
       "  'strategic marketing',\n",
       "  'strategic procurement',\n",
       "  'team management',\n",
       "  'custom law',\n",
       "  'supply chain',\n",
       "  'supply chain management',\n",
       "  'data visualization',\n",
       "  'com',\n",
       "  'ERP',\n",
       "  'MRO',\n",
       "  'ERP',\n",
       "  'procurement',\n",
       "  'sourcing',\n",
       "  'scheduling',\n",
       "  'procurement',\n",
       "  'supplier relationship',\n",
       "  'purchasing',\n",
       "  'warehousing',\n",
       "  'logistics',\n",
       "  'procurement',\n",
       "  'sourcing corporation',\n",
       "  'procurement',\n",
       "  'forecasting',\n",
       "  'world wide',\n",
       "  'procurement',\n",
       "  'supplier relationship',\n",
       "  'procurement',\n",
       "  'logistic support',\n",
       "  'communications',\n",
       "  'coordinating',\n",
       "  'management functions',\n",
       "  'team performance',\n",
       "  'procurement',\n",
       "  'implementation system',\n",
       "  'management',\n",
       "  'procurement',\n",
       "  'global market',\n",
       "  'gather information',\n",
       "  'procurement',\n",
       "  'planning',\n",
       "  'supported analysis',\n",
       "  'operations',\n",
       "  'forecasting',\n",
       "  'coordinating',\n",
       "  'logistics support',\n",
       "  'performing analysis',\n",
       "  'requirements analysis',\n",
       "  'logistics support',\n",
       "  'sourcing',\n",
       "  'procurement',\n",
       "  'construction materials',\n",
       "  'batteries',\n",
       "  'foods',\n",
       "  'beverages',\n",
       "  'oils',\n",
       "  'spare parts',\n",
       "  'cost estimates',\n",
       "  'milestones',\n",
       "  'project coordinator',\n",
       "  'procurement',\n",
       "  'logistics',\n",
       "  'statistics',\n",
       "  'arab',\n",
       "  'government projects',\n",
       "  'life cycle',\n",
       "  'conceptualization',\n",
       "  'planning',\n",
       "  'assets register',\n",
       "  'assets register',\n",
       "  'inventories vendor',\n",
       "  'sourcing',\n",
       "  'procurement',\n",
       "  'warehousing',\n",
       "  'stock checks',\n",
       "  'filing',\n",
       "  'warehousing',\n",
       "  'logistics',\n",
       "  'management',\n",
       "  'dec',\n",
       "  'logistics',\n",
       "  'aviation',\n",
       "  'logistics',\n",
       "  'management',\n",
       "  'procurement',\n",
       "  'sourcing',\n",
       "  'demand forecasts',\n",
       "  'alliances strategic',\n",
       "  'warehousing',\n",
       "  'supplier relationship',\n",
       "  'languages',\n",
       "  'languages english',\n",
       "  'languages english',\n",
       "  'french',\n",
       "  'arabic',\n",
       "  'statistics',\n",
       "  'logistics']}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##skill Rule based\n",
    "skills = rulebasedSkills(cv_text)\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6adb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b50c6544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.my_component(doc)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @Language.component(\"test2_component\")\n",
    "# def my_component(doc):\n",
    "    \n",
    "#     length=len(str(doc))\n",
    "#     return length\n",
    "\n",
    "# nlp_trf_model.add_pipe(\"test2_component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a31609ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'test2_component']\n"
     ]
    }
   ],
   "source": [
    "# print(nlp_trf_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6bc390f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_trf_model.remove_pipe(\"rulebasedSkills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2146fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### TEST on DATA\n",
    "# import docx2txt\n",
    "# Cv_PathToTest=\"./ResumeData/CV Wyc.docx\"\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "\n",
    "# # text_file = open(\"/home/user/HR Analytics/Custom_NER/Java_Developer/Java_Developer_1.txt\",\"r\")\n",
    "# # cv_text = text_file.read()\n",
    "# # print(data)\n",
    "\n",
    "# doc = nlp_trf_model(cv_text) # input sample text\n",
    "# print(\"====\",doc)\n",
    "# #check with only entity result \n",
    "# for word in doc.ents:\n",
    "#     print(word.text,\"==========>\",word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8d76ff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Location',\n",
       " 'Total_Experience',\n",
       " 'college_name',\n",
       " 'company_names',\n",
       " 'degree',\n",
       " 'designation',\n",
       " 'name',\n",
       " 'skills')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_lg_model.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db285ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "06e15b7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'ner_custom' already exists in pipeline. Existing names: ['tok2vec', 'ner_custom', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [223]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnlp_lg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner_custom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustnlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mafter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/spacy/language.py:776\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    774\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n\u001b[0;32m--> 776\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE007\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names))\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# We're loading the component from a model. After loading the\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# component, we know its real factory name\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     pipe_component, factory_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe_from_source(\n\u001b[1;32m    781\u001b[0m         factory_name, source, name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    782\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'ner_custom' already exists in pipeline. Existing names: ['tok2vec', 'ner_custom', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer']"
     ]
    }
   ],
   "source": [
    "nlp_lg_model.add_pipe(\n",
    "    \"ner\",\n",
    "    name=\"ner_custom\",\n",
    "    source=custnlp,\n",
    "    after=\"ner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "049fe3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f93c34acba0>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_lg_model.remove_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "61fffd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'ner_custom', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "print(nlp_lg_model.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b007f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alternative ==========> skills\n",
      "locations ==========> skills\n",
      ") ==========> skills\n",
      "– ==========> skills\n"
     ]
    }
   ],
   "source": [
    "##### TEST on DATA\n",
    "import docx2txt\n",
    "# Cv_PathToTest=\"./ResumeData/CV Wyc.docx\"\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "Cv_PathToTest=\"./ResumeData/RMS_DATA/Resume_Download/72699.docx\" #select resume to test 1Mawanda Swaib CV.docx\n",
    "\n",
    "cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# text_file = open(\"/home/user/HR Analytics/Custom_NER/Java_Developer/Java_Developer_1.txt\",\"r\")\n",
    "# cv_text = text_file.read()\n",
    "# print(data)\n",
    "\n",
    "doc = nlp_lg_model(cv_text) # input sample text\n",
    "# print(\"====\",doc.ents)\n",
    "# check with only entity result \n",
    "for word in doc.ents:\n",
    "    print(word.text,\"==========>\",word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    ", ==========> company_names\n",
    "back ==========> Location\n",
    "- ==========> company_names\n",
    "put ==========> company_names\n",
    ", ==========> company_names\n",
    ", Frame ==========> company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d33a6320",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [243]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m cv_text \u001b[38;5;241m=\u001b[39m docx2txt\u001b[38;5;241m.\u001b[39mprocess(Cv_PathToTest)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Cv_PathToTest=\"./ResumeData/RMS_DATA/Resume_Download/1Mawanda Swaib CV.docx\" #select resume to test\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# cv_text = docx2txt.process(Cv_PathToTest)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# text_file = open(\"/home/user/HR Analytics/Custom_NER/Java_Developer/Java_Developer_1.txt\",\"r\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# cv_text = text_file.read()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_trf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_text\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# input sample text\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(\"====\",doc.ents)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# check with only entity result \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/spacy/language.py:1022\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1022\u001b[0m     \u001b[43merror_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE005\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/spacy/util.py:1617\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[0;32m-> 1617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/spacy/language.py:1017\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1017\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "Input \u001b[0;32mIn [241]\u001b[0m, in \u001b[0;36mrulebasedSkills\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     17\u001b[0m skills\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     18\u001b[0m SkillResult\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m---> 20\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[43mskill_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m full_matches\u001b[38;5;241m=\u001b[39mannotations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_matches\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m ngram_scored\u001b[38;5;241m=\u001b[39mannotations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngram_scored\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/skillNer/skill_extractor_class.py:116\u001b[0m, in \u001b[0;36mSkillExtractor.annotate\u001b[0;34m(self, text, tresh)\u001b[0m\n\u001b[1;32m    113\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranlsator_func(text)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# create text object\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m text_obj \u001b[38;5;241m=\u001b[39m \u001b[43mText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# get matches\u001b[39;00m\n\u001b[1;32m    118\u001b[0m skills_full, text_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_getters\u001b[38;5;241m.\u001b[39mget_full_match_skills(\n\u001b[1;32m    119\u001b[0m     text_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatchers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_matcher\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/skillNer/text_class.py:148\u001b[0m, in \u001b[0;36mText.__init__\u001b[0;34m(self, text, nlp)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# transformed text: lower + punctuation + extra space\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# this is the version of text that we will be working with\u001b[39;00m\n\u001b[1;32m    140\u001b[0m cleaner \u001b[38;5;241m=\u001b[39m Cleaner(\n\u001b[1;32m    141\u001b[0m     include_cleaning_functions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     to_lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    146\u001b[0m )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformed_text \u001b[38;5;241m=\u001b[39m \u001b[43mcleaner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# abv version\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabv_text \u001b[38;5;241m=\u001b[39m cleaner(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/skillNer/cleaner.py:304\u001b[0m, in \u001b[0;36mCleaner.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cleaning_name \u001b[38;5;129;01min\u001b[39;00m dict_cleaning_functions\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cleaning_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_cleaning_functions:\n\u001b[0;32m--> 304\u001b[0m             text \u001b[38;5;241m=\u001b[39m \u001b[43mdict_cleaning_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcleaning_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/anaconda3/envs/NER/lib/python3.8/site-packages/skillNer/cleaner.py:50\u001b[0m, in \u001b[0;36mremove_punctuation\u001b[0;34m(text, list_punctuations)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"To Remove punctuation from a given text.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mHello there  I am SkillNer  Annoation  annotation  annotation\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m punc \u001b[38;5;129;01min\u001b[39;00m list_punctuations:\n\u001b[0;32m---> 50\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(punc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# use .strip() to remove extra space in the begining/end of the text\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "##### TEST on DATA\n",
    "import docx2txt\n",
    "Cv_PathToTest=\"./ResumeData/CV Wyc.docx\"\n",
    "cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# Cv_PathToTest=\"./ResumeData/RMS_DATA/Resume_Download/1Mawanda Swaib CV.docx\" #select resume to test\n",
    "\n",
    "# cv_text = docx2txt.process(Cv_PathToTest)\n",
    "# text_file = open(\"/home/user/HR Analytics/Custom_NER/Java_Developer/Java_Developer_1.txt\",\"r\")\n",
    "# cv_text = text_file.read()\n",
    "# print(data)\n",
    "\n",
    "doc = nlp_trf_model(cv_text) # input sample text\n",
    "# print(\"====\",doc.ents)\n",
    "# check with only entity result \n",
    "for word in doc.ents:\n",
    "    print(word.text,\"==========>\",word.label_)\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c90378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NER] *",
   "language": "python",
   "name": "conda-env-NER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
